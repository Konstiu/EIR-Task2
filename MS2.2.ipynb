{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2ccad7-33bd-44b8-abb3-a0517c683e88",
   "metadata": {},
   "source": [
    "# MS 2.2- Group 10\n",
    "\n",
    "**Members**: Niklas GrÃ¼ner (12217059), Konstantin Unterweger (12222169), Martin Harhammer (12221683)\n",
    "\n",
    "\n",
    "The following sections describe and implement an attempt to Audio Identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f54f9-5a1c-422d-a1b0-4a724d15679b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d41eb8e-5435-4235-be1c-2d82cf56a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import librosa\n",
    "#from scipy import signal\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "\n",
    "sys.path.append('..')\n",
    "import libfmp.b\n",
    "import libfmp.c2\n",
    "import libfmp.c6\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a404e0-ae94-4a93-90f6-1cb6f5d912ce",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Here we define all functions that, we later need to compute spectrograms, constellation maps, the hashes and perform the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37de749-9b67-444b-b8c0-51dd0c3b3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filenames(directory):\n",
    "    filenames = []\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Create the full path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if it is a file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Add to the dictionary, using the filename as the key\n",
    "            filenames.append(file_path)\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef06df91-9bc6-41cb-81ef-cd227c10a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrogram(fn_wav, Fs=22050, N=2048, H=1024, bin_max=128, frame_max=None, duration=None):\n",
    "    x, Fs = librosa.load(fn_wav, sr=Fs)\n",
    "    x_duration = len(x) / Fs\n",
    "    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n",
    "    if bin_max is None:\n",
    "        bin_max = X.shape[0]\n",
    "    if frame_max is None:\n",
    "        frame_max = X.shape[0]\n",
    "    Y = np.abs(X[:bin_max, :frame_max])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2700908-b75b-40c3-8248-156855ceebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_constellation_map(Y, dist_freq=7, dist_time=7, thresh=0.01):\n",
    "    \"\"\"Compute constellation map (implementation using image processing)\n",
    "\n",
    "    Notebook: C7/C7S1_AudioIdentification.ipynb\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): Spectrogram (magnitude)\n",
    "        dist_freq (int): Neighborhood parameter for frequency direction (kappa) (Default value = 7)\n",
    "        dist_time (int): Neighborhood parameter for time direction (tau) (Default value = 7)\n",
    "        thresh (float): Threshold parameter for minimal peak magnitude (Default value = 0.01)\n",
    "\n",
    "    Returns:\n",
    "        Cmap (np.ndarray): Boolean mask for peak structure (same size as Y)\n",
    "    \"\"\"\n",
    "    result = ndimage.maximum_filter(Y, size=[2*dist_freq+1, 2*dist_time+1], mode='constant')\n",
    "    Cmap = np.logical_and(Y == result, result > thresh)\n",
    "    return Cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530cacda-d4b4-4302-aa6c-1dc2ea160662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def compute_constellation_map_single(args):\n",
    "    \"\"\"Compute the constellation map for a single file.\"\"\"\n",
    "    filename, dist_freq, dist_time = args\n",
    "    spectrogram = compute_spectrogram(filename)  # Perform I/O and computation\n",
    "    constellation_map = compute_constellation_map(spectrogram, dist_freq, dist_time)\n",
    "    return filename, constellation_map\n",
    "\n",
    "def compute_constellation_maps(filenames, dist_freq, dist_time):\n",
    "    \"\"\"Compute constellation maps using multithreading.\"\"\"\n",
    "    # Prepare arguments for each file\n",
    "    args = [(filename, dist_freq, dist_time) for filename in filenames]\n",
    "    \n",
    "    # Use ThreadPoolExecutor for multithreading\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(compute_constellation_map_single, args)\n",
    "    \n",
    "    # Convert results to a dictionary\n",
    "    Cmaps = dict(results)\n",
    "    return Cmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa3ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bc56ad-2af7-49df-baa3-f4adfc094d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_hashes_from_spectrogram(filename, dist_freq, dist_time, \n",
    "                                    time_min_offset, time_max_offset, \n",
    "                                    freq_min_offset, freq_max_offset):\n",
    "    \"\"\"\n",
    "    Compute hashes for a single track directly from its spectrogram.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to the audio file.\n",
    "        dist_freq (int): Neighborhood parameter for frequency direction.\n",
    "        dist_time (int): Neighborhood parameter for time direction.\n",
    "        time_min_offset, time_max_offset, freq_min_offset, freq_max_offset (int): \n",
    "            Bounds for the \"target zone\" around an anchor point.\n",
    "\n",
    "    Returns:\n",
    "        list: [(hash, track_name, time_offset), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    global count\n",
    "    global count2\n",
    "    spectrogram = compute_spectrogram(filename)\n",
    "    cmap = compute_constellation_map(spectrogram, dist_freq, dist_time)\n",
    "    \n",
    "    # Convert the boolean map to a list of points (freq, time), sorted by time\n",
    "    point_list = sorted(np.argwhere(cmap).tolist(), key=lambda x: x[1])\n",
    "    hashes = []\n",
    "    del cmap\n",
    "    del spectrogram\n",
    "    \n",
    "    for anchor in point_list:\n",
    "        target_points = get_target_zone_points(\n",
    "            anchor, \n",
    "            point_list,\n",
    "            time_min_offset=time_min_offset,\n",
    "            time_max_offset=time_max_offset,\n",
    "            freq_min_offset=freq_min_offset,\n",
    "            freq_max_offset=freq_max_offset\n",
    "        )\n",
    "        for target_point in target_points:\n",
    "            h = compute_hash(anchor, target_point)\n",
    "            hashes.append((h, int(extract_numeric_id(filename)), anchor[1]))\n",
    "    \n",
    "    count += 1\n",
    "    count2 += 1\n",
    "    if (count % 1000) == 0:\n",
    "        gc.collect()\n",
    "        print(f\"Processed {count} tracks\")\n",
    "\n",
    "    return hashes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_database(\n",
    "    directory,\n",
    "    dist_freq=11,\n",
    "    dist_time=3,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a fingerprint database directly from spectrograms using multithreading.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory containing audio files.\n",
    "        dist_freq, dist_time (int): Constellation map neighborhood parameters.\n",
    "        time_min_offset, time_max_offset, freq_min_offset, freq_max_offset (int): \n",
    "            Bounds for the \"target zone\" around an anchor point.\n",
    "\n",
    "    Returns:\n",
    "        database (defaultdict): {hash: [(track_name, time_offset), ...]}\n",
    "    \"\"\"\n",
    "    tracks = load_filenames(directory)\n",
    "    \n",
    "    # Prepare arguments for each file\n",
    "    args = [\n",
    "        (filename, dist_freq, dist_time, time_min_offset, time_max_offset, freq_min_offset, freq_max_offset) \n",
    "        for filename in tracks\n",
    "    ]\n",
    "    \n",
    "    database = defaultdict(list)\n",
    "    global count2\n",
    "\n",
    "    # Use multithreading to compute hashes for all tracks\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for result in executor.map(lambda x: compute_hashes_from_spectrogram(*x), args):\n",
    "            for h, track_name, time_offset in result:\n",
    "                database[h].append((track_name, time_offset))\n",
    "\n",
    "            if(count2 > 1000):\n",
    "                count2 = 0\n",
    "                save_partial_database(database, \"partial_database2.pkl\")\n",
    "                database.clear()\n",
    "    \n",
    "    save_partial_database(database, \"partial_database2.pkl\")\n",
    "    database.clear()\n",
    "    return database\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4636f6d5-0fbe-46b6-9222-17b58ab9f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(anchor, target):\n",
    "    \"\"\"Generate a 32-bit hash.\"\"\"\n",
    "    f1, t1 = anchor\n",
    "    f2, t2 = target\n",
    "    dt = t2 - t1\n",
    "    return (f1 & 0x3FF) | ((f2 & 0x3FF) << 10) | ((dt & 0xFFF) << 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c2067f6-84be-431a-913b-0cec6237fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_zone_points(anchor, point_list,\n",
    "                           time_min_offset, time_max_offset,\n",
    "                           freq_min_offset, freq_max_offset):\n",
    "    \"\"\"\n",
    "    Finds points in 'point_list' that lie within time [t1 + time_min_offset, t1 + time_max_offset]\n",
    "    and frequency [f1 + freq_min_offset, f1 + freq_max_offset].\n",
    "    \n",
    "    anchor: (f1, t1) or (t1, f1) â whichever convention you are using.\n",
    "    point_list: list of (f, t) or (t, f) â must be sorted by the time coordinate if we want to break early.\n",
    "    time_min_offset, time_max_offset: how far in time we look relative to anchor's time t1.\n",
    "    freq_min_offset, freq_max_offset: how far in frequency we look relative to anchor's freq f1.\n",
    "    \"\"\"\n",
    "    f1, t1 = anchor\n",
    "    \n",
    "    # Time bounds\n",
    "    t_min = t1 + time_min_offset\n",
    "    t_max = t1 + time_max_offset\n",
    "    \n",
    "    # Frequency bounds\n",
    "    f_min = f1 + freq_min_offset\n",
    "    f_max = f1 + freq_max_offset\n",
    "    \n",
    "    target_zone_points = []\n",
    "    for (f2, t2) in point_list:\n",
    "        # If the list is sorted by time and t2 > t_max, we can break early.\n",
    "        if t2 > t_max:\n",
    "            break\n",
    "        \n",
    "        if t_min < t2 <= t_max and f_min <= f2 <= f_max:\n",
    "            target_zone_points.append((f2, t2))\n",
    "    \n",
    "    return target_zone_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3fd158-c0ba-461c-95f1-ecec90d31fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def extract_numeric_id(filename):\n",
    "    \"\"\"\n",
    "    Example: \n",
    "      - 'queries/1269810_original.mp3' => '1269810'\n",
    "      - 'tracks/1269810.mp3'           => '1269810'\n",
    "    Adjust to your naming conventions as needed.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e7874a6-1d57-42a5-b18e-eef2fd2a243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_for_query(\n",
    "    query_name,\n",
    "    cmap,\n",
    "    database,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a query's spectrogram cmap (2D Boolean array),\n",
    "    find the best matching track in the database of fingerprints.\n",
    "\n",
    "    Returns:\n",
    "        best_track: str or None\n",
    "        best_delta: int or None\n",
    "        best_count: int\n",
    "        point_list: list of non-zero (freq,time) points in the query\n",
    "    \"\"\"\n",
    "    # Convert the boolean array to a list of points (freq, time)\n",
    "    point_list = sorted(np.argwhere(cmap).tolist(), key=lambda x: x[1])\n",
    "\n",
    "    # Dictionary of track_name -> (offset -> count)\n",
    "    matches = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # For each point in the query, find matching points in the database\n",
    "    for anchor in point_list:\n",
    "        target_points = get_target_zone_points(\n",
    "            anchor,\n",
    "            point_list,\n",
    "            time_min_offset=time_min_offset,\n",
    "            time_max_offset=time_max_offset,\n",
    "            freq_min_offset=freq_min_offset,\n",
    "            freq_max_offset=freq_max_offset\n",
    "        )\n",
    "\n",
    "        for target_point in target_points:\n",
    "            h = compute_hash(anchor, target_point)\n",
    "\n",
    "            # If our hash is found in the database\n",
    "            if h in database:\n",
    "                # database[h] = list of (track_name, track_offset)\n",
    "                for track_name, track_offset in database[h]:\n",
    "                    delta_offset = track_offset - anchor[1]\n",
    "                    matches[track_name][delta_offset] += 1\n",
    "    \n",
    "    # Find the best match with the highest count\n",
    "    best_track = None\n",
    "    best_delta = None\n",
    "    best_count = 0\n",
    "    \n",
    "    for track_name, offset_counts in matches.items():\n",
    "        for delta_offset, count in offset_counts.items():\n",
    "            if count > best_count:\n",
    "                best_count = count\n",
    "                best_track = track_name\n",
    "                best_delta = delta_offset\n",
    "\n",
    "    return best_track, best_delta, best_count, point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb364987-8706-4dc9-9c89-cdce29db2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_chunks_from_pickle(filename):\n",
    "    \"\"\"Load all chunks from a single pickle file into memory.\"\"\"\n",
    "    chunks = []\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                chunks.append(pickle.load(f))  # Load each chunk and append to the list\n",
    "            except EOFError:\n",
    "                break  # Stop when end of file is reached\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0451243-5d8d-4340-ad90-ea8ffb42f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_queries_with_chunks(\n",
    "    directory,\n",
    "    database_file,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Match queries against a database loaded incrementally in chunks.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the query files.\n",
    "        database_file (str): Path to the serialized database file.\n",
    "        time_min_offset, time_max_offset, freq_min_offset, freq_max_offset (int): \n",
    "            Bounds for matching.\n",
    "\n",
    "    Returns:\n",
    "        results (list): List of query match results.\n",
    "    \"\"\"\n",
    "    queries = load_filenames(directory)  # Load all query filenames\n",
    "    results = []\n",
    "    num_correct = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    cmaps = {}\n",
    "    \n",
    "    for query_name in queries:\n",
    "        _, cmap = compute_constellation_map_single((query_name, 11, 3))\n",
    "        cmaps[query_name]=cmap\n",
    "\n",
    "    bestmatches = {}\n",
    "    for database_chunk in load_partial_database(database_file):\n",
    "        print(\"chunk\")\n",
    "        for query_name in queries:\n",
    "            track, delta, count, points = find_best_match_for_query(\n",
    "                query_name,\n",
    "                cmaps[query_name],\n",
    "                database_chunk,\n",
    "                time_min_offset=time_min_offset,\n",
    "                time_max_offset=time_max_offset,\n",
    "                freq_min_offset=freq_min_offset,\n",
    "                freq_max_offset=freq_max_offset\n",
    "            )\n",
    "            if query_name in bestmatches: \n",
    "                if bestmatches[query_name][2] < count:\n",
    "                    bestmatches[query_name] = track, delta, count, points\n",
    "            else: \n",
    "                bestmatches[query_name] = track, delta, count, points\n",
    "\n",
    "\n",
    "\n",
    "    # Print final results\n",
    "    count3 = 0\n",
    "    for query_name in queries:\n",
    "        if int(extract_numeric_id(query_name)) == bestmatches[query_name][0]:\n",
    "            count3 += 1\n",
    "        print(query_name, bestmatches[query_name][0])\n",
    "    print(count3)\n",
    "\n",
    "    return bestmatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b042d3-89d4-45f3-a518-36663b327801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partial_database(filename):\n",
    "    \"\"\"Load chunks from a pickle file incrementally.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                yield pickle.load(f)  # Yield one chunk at a time\n",
    "            except EOFError:\n",
    "                break  # Stop when end of file is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8fa4b0-274a-4f67-b712-017a5dee45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_queries(\n",
    "    directory,\n",
    "    database,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    - Matches each query in cmaps_Q to the best track in the database.\n",
    "    :param cmaps_Q: dict {query_name: 2D numpy array (spectrogram boolean map)}\n",
    "    :param database: dict {hash: [(track_name, track_offset), ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    queries = load_filenames(directory) # load all track filenames\n",
    "        \n",
    "    results = []\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Collect queries in a list to iterate consistently\n",
    "    total_queries = len(queries)\n",
    "\n",
    "    \n",
    "\n",
    "    for query_name in queries:\n",
    "        starttime = time.time()\n",
    "        \n",
    "        _, cmap = compute_constellation_map_single((query_name, 11, 3))       \n",
    "\n",
    "        best_track = None\n",
    "        best_delta = None\n",
    "        best_count = 0\n",
    "        point_list = []\n",
    "        \n",
    "        for database_chunk in load_partial_database(database_file):\n",
    "            track, delta, count, points = find_best_match_for_query(\n",
    "                query_name,\n",
    "                cmap,\n",
    "                database_chunk,\n",
    "                time_min_offset=time_min_offset,\n",
    "                time_max_offset=time_max_offset,\n",
    "                freq_min_offset=freq_min_offset,\n",
    "                freq_max_offset=freq_max_offset\n",
    "            )\n",
    "\n",
    "            # Update the best match if this chunk has a better match\n",
    "            if count > best_count:\n",
    "                best_track, best_delta, best_count, point_list = track, delta, count, points\n",
    "\n",
    "\n",
    "        endtime = time.time()\n",
    "        \n",
    "\n",
    "        query_id = extract_numeric_id(query_name)\n",
    "        track_id = extract_numeric_id(best_track) if best_track else None\n",
    "        correct = (query_id == track_id)\n",
    "        if correct:\n",
    "            num_correct += 1\n",
    "\n",
    "        # Store the result\n",
    "        results.append((query_name, best_track, best_delta, best_count, correct, (endtime-starttime)*1000))\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Print final results\n",
    "    # ----------------------------------------------------------------------\n",
    "    accuracy = num_correct / total_queries if total_queries else 0\n",
    "    print(\"\\nMATCHING RESULTS:\")\n",
    "    sum_duration = 0\n",
    "    for qname, tname, offset, count, is_correct, duration in results:\n",
    "        print(f\"Query: {qname} => Best match: {tname}, Time: {duration} ms, Offset: {offset}, Count: {count}, Correct: {is_correct}\")\n",
    "        sum_duration += duration\n",
    "\n",
    "    print(f\"\\nCorrect matches: {num_correct}/{total_queries}\")\n",
    "    print(f\"Average query time: {sum_duration/len(results)} ms\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb22459-1b56-4506-ba7b-2b617fec7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_partial_database(database, filename):\n",
    "    \"\"\"Append partial data to a pickle file.\"\"\"\n",
    "    with open(filename, \"ab\") as f:  # Append mode\n",
    "        pickle.dump(database, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9cf99-83c1-4495-aa81-895789d3c546",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6e30e-bdb3-4691-9050-a712fcf7b579",
   "metadata": {},
   "source": [
    "## Define configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cecf6808-828d-4ea2-9133-8fa244391291",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    \"time_min_offset\": 0,\n",
    "    \"time_max_offset\": 50,\n",
    "    \"freq_min_offset\": -10,\n",
    "    \"freq_max_offset\": 30\n",
    "}\n",
    "\n",
    "config_2 = {\n",
    "    \"time_min_offset\": 0,\n",
    "    \"time_max_offset\": 30,\n",
    "    \"freq_min_offset\": 0,\n",
    "    \"freq_max_offset\": 25\n",
    "}\n",
    "\n",
    "config_3 = {\n",
    "    \"time_min_offset\": 10,\n",
    "    \"time_max_offset\": 40,\n",
    "    \"freq_min_offset\": -20,\n",
    "    \"freq_max_offset\": 20\n",
    "}\n",
    "\n",
    "config_4 = {\n",
    "    \"time_min_offset\": 0,\n",
    "    \"time_max_offset\": 50,\n",
    "    \"freq_min_offset\": -10,\n",
    "    \"freq_max_offset\": 30\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554b41e-ef1f-446d-aa7e-19af3ccba652",
   "metadata": {},
   "source": [
    "## Build the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b7e400-1f3b-4327-89ab-6d64f32c6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 tracks\n",
      "Processed 2000 tracks\n",
      "Processed 3000 tracks\n",
      "Processed 4000 tracks\n",
      "Processed 5000 tracks\n",
      "Processed 6000 tracks\n",
      "Processed 7000 tracks\n",
      "Processed 8000 tracks\n",
      "Processed 9000 tracks\n",
      "Processed 10000 tracks\n",
      "Processed 11000 tracks\n",
      "Processed 12000 tracks\n",
      "Processed 13000 tracks\n",
      "Processed 14000 tracks\n",
      "Processed 15000 tracks\n",
      "Processed 16000 tracks\n",
      "Processed 17000 tracks\n",
      "Processed 18000 tracks\n",
      "Processed 19000 tracks\n",
      "Processed 20000 tracks\n",
      "Processed 21000 tracks\n",
      "Processed 22000 tracks\n",
      "Processed 23000 tracks\n",
      "Processed 24000 tracks\n",
      "Processed 25000 tracks\n",
      "Processed 26000 tracks\n",
      "Processed 27000 tracks\n",
      "Processed 28000 tracks\n",
      "Processed 29000 tracks\n",
      "Processed 30001 tracks\n",
      "Processed 31000 tracks\n",
      "Processed 32000 tracks\n",
      "Processed 33000 tracks\n",
      "2732.0914878845215\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "database_1 = build_database(\"tracks\", **config_1)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147d027-522b-48dd-8507-797722ff18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_2 = build_database(\"tracks\", **config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a946fddc-2a3b-4edd-9166-fe1337b3a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 tracks\n",
      "Processed 2000 tracks\n",
      "Processed 3000 tracks\n",
      "Processed 4000 tracks\n",
      "Processed 5000 tracks\n",
      "Processed 6000 tracks\n",
      "Processed 7000 tracks\n",
      "Processed 8000 tracks\n",
      "Processed 9000 tracks\n",
      "Processed 10000 tracks\n",
      "Processed 11000 tracks\n",
      "Processed 12000 tracks\n",
      "Processed 13000 tracks\n",
      "Processed 14000 tracks\n",
      "Processed 15000 tracks\n",
      "Processed 16000 tracks\n",
      "Processed 17000 tracks\n",
      "Processed 18000 tracks\n",
      "Processed 19000 tracks\n",
      "Processed 20000 tracks\n",
      "Processed 21000 tracks\n",
      "Processed 22000 tracks\n",
      "Processed 23000 tracks\n",
      "Processed 24000 tracks\n",
      "Processed 25000 tracks\n",
      "Processed 26000 tracks\n",
      "Processed 27000 tracks\n",
      "Processed 28000 tracks\n",
      "Processed 29000 tracks\n",
      "Processed 30000 tracks\n",
      "Processed 31000 tracks\n",
      "Processed 32000 tracks\n",
      "Processed 33000 tracks\n",
      "Processed 34000 tracks\n",
      "2110.072087287903\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "database_3 = build_database(\"tracks\", **config_3)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc4578-d745-4daf-90ea-07a53c3dcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_4 = build_database(\"tracks\", **config_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57670e89-9d39-4e92-a9a4-cf674a13f30a",
   "metadata": {},
   "source": [
    "### Store the databases on the harddrive (skip if not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820b0f6-341a-4779-b941-1c2007cfc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "try:\n",
    "    database_1\n",
    "    with open(\"database_1.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_1, file)\n",
    "except NameError:\n",
    "    print(\"database_1 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_2\n",
    "    with open(\"database_2.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_2, file)\n",
    "except NameError:\n",
    "    print(\"database_2 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_2\n",
    "    with open(\"database_3.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_2, file)\n",
    "except NameError:\n",
    "    print(\"database_3 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_4\n",
    "    with open(\"database_4.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_4, file)\n",
    "except NameError:\n",
    "    print(\"database_4 doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01437-7b86-49f8-94df-b8bcc61367a2",
   "metadata": {},
   "source": [
    "# Task 2: Audio Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564c111-4dc1-44df-8fea-6f73b4a3e2a4",
   "metadata": {},
   "source": [
    "### Load the Databases into memory (skip if not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03698546-ce85-434d-bb94-4fb192a0344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database_2 doesn't exist\n",
      "database_3 doesn't exist\n",
      "database_4 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"database_1.pkl\", \"rb\") as file:\n",
    "        database_1 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_1 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_2.pkl\", \"rb\") as file:\n",
    "        database_2 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_2 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_3.pkl\", \"rb\") as file:\n",
    "        database_3 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_3 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_4.pkl\", \"rb\") as file:\n",
    "        database_4 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_4 doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2a224-01d3-4882-940d-1dc0ad204d07",
   "metadata": {},
   "source": [
    "### Match all test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "554723cb-6157-45c4-a28b-20f1e2da7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "queries/1192210_noise.mp3 1192210\n",
      "queries/91810_coding.mp3 91810\n",
      "queries/1084710_original.mp3 1084710\n",
      "queries/242610_mobile.mp3 900733\n",
      "queries/53310_original.mp3 53310\n",
      "queries/1192210_mobile.mp3 1192210\n",
      "queries/1147910_mobile.mp3 1362117\n",
      "queries/242610_noise.mp3 242610\n",
      "queries/152310_mobile.mp3 152310\n",
      "queries/1227910_original.mp3 1227910\n",
      "queries/963810_noise.mp3 963810\n",
      "queries/887210_noise.mp3 887210\n",
      "queries/91810_noise.mp3 91810\n",
      "queries/1084710_coding.mp3 1084710\n",
      "queries/1269810_mobile.mp3 1269810\n",
      "queries/1390710_mobile.mp3 1390710\n",
      "queries/1400510_noise.mp3 153617\n",
      "queries/91810_original.mp3 91810\n",
      "queries/1084710_noise.mp3 1084710\n",
      "queries/1249910_original.mp3 1249910\n",
      "queries/1192210_coding.mp3 1192210\n",
      "queries/262010_coding.mp3 262010\n",
      "queries/262010_original.mp3 262010\n",
      "queries/1390710_noise.mp3 1390710\n",
      "queries/1147910_original.mp3 1147910\n",
      "queries/219510_noise.mp3 219510\n",
      "queries/136810_coding.mp3 136810\n",
      "queries/1400510_coding.mp3 1400510\n",
      "queries/887210_mobile.mp3 887210\n",
      "queries/1227910_noise.mp3 1227910\n",
      "queries/119410_mobile.mp3 119410\n",
      "queries/1269810_original.mp3 1269810\n",
      "queries/53310_noise.mp3 53310\n",
      "queries/136810_mobile.mp3 136304\n",
      "queries/963810_mobile.mp3 963810\n",
      "queries/1390710_coding.mp3 1390710\n",
      "queries/1400510_mobile.mp3 1271442\n",
      "queries/119410_coding.mp3 119410\n",
      "queries/219510_original.mp3 219510\n",
      "queries/94710_noise.mp3 94710\n",
      "queries/963810_original.mp3 963810\n",
      "queries/1192210_original.mp3 1192210\n",
      "queries/152310_coding.mp3 152310\n",
      "queries/94710_original.mp3 94710\n",
      "queries/219510_mobile.mp3 1284439\n",
      "queries/810_coding.mp3 810\n",
      "queries/91810_mobile.mp3 91810\n",
      "queries/810_noise.mp3 810\n",
      "queries/219510_coding.mp3 219510\n",
      "queries/1400510_original.mp3 1400510\n",
      "queries/1227910_mobile.mp3 37235\n",
      "queries/810_original.mp3 810\n",
      "queries/152310_noise.mp3 152310\n",
      "queries/1269810_noise.mp3 1269810\n",
      "queries/53310_coding.mp3 53310\n",
      "queries/1249910_mobile.mp3 1249910\n",
      "queries/94710_coding.mp3 94710\n",
      "queries/1390710_original.mp3 1390710\n",
      "queries/53310_mobile.mp3 53310\n",
      "queries/1249910_coding.mp3 1249910\n",
      "queries/152310_original.mp3 152310\n",
      "queries/242610_coding.mp3 242610\n",
      "queries/262010_noise.mp3 262010\n",
      "queries/1269810_coding.mp3 1269810\n",
      "queries/1147910_coding.mp3 1147910\n",
      "queries/119410_original.mp3 119410\n",
      "queries/810_mobile.mp3 1093545\n",
      "queries/119410_noise.mp3 119410\n",
      "queries/887210_original.mp3 887210\n",
      "queries/1084710_mobile.mp3 321050\n",
      "queries/1147910_noise.mp3 1147910\n",
      "queries/136810_original.mp3 136810\n",
      "queries/887210_coding.mp3 887210\n",
      "queries/1249910_noise.mp3 1249910\n",
      "queries/94710_mobile.mp3 94710\n",
      "queries/262010_mobile.mp3 354914\n",
      "queries/242610_original.mp3 242610\n",
      "queries/136810_noise.mp3 136810\n",
      "queries/1227910_coding.mp3 1227910\n",
      "queries/963810_coding.mp3 963810\n",
      "70\n",
      "195.1383740901947\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "matches_1 = match_queries_with_chunks(\"queries\", \"partial_database2.pkl\", **config_1)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8dd84-c409-4f64-a133-d9f7309884b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = match_queries(\"queries\", database_2, **config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f26c43c-047d-4d50-8286-efb526123346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "queries/1192210_noise.mp3 1192210\n",
      "queries/91810_coding.mp3 91810\n",
      "queries/1084710_original.mp3 1084710\n",
      "queries/242610_mobile.mp3 1352901\n",
      "queries/53310_original.mp3 53310\n",
      "queries/1192210_mobile.mp3 1192210\n",
      "queries/1147910_mobile.mp3 1066916\n",
      "queries/242610_noise.mp3 242610\n",
      "queries/152310_mobile.mp3 152310\n",
      "queries/1227910_original.mp3 1227910\n",
      "queries/963810_noise.mp3 963810\n",
      "queries/887210_noise.mp3 887210\n",
      "queries/91810_noise.mp3 91810\n",
      "queries/1084710_coding.mp3 1084710\n",
      "queries/1269810_mobile.mp3 1269810\n",
      "queries/1390710_mobile.mp3 1390710\n",
      "queries/1400510_noise.mp3 1146535\n",
      "queries/91810_original.mp3 91810\n",
      "queries/1084710_noise.mp3 1084710\n",
      "queries/1249910_original.mp3 1249910\n",
      "queries/1192210_coding.mp3 1192210\n",
      "queries/262010_coding.mp3 262010\n",
      "queries/262010_original.mp3 262010\n",
      "queries/1390710_noise.mp3 1390710\n",
      "queries/1147910_original.mp3 1147910\n",
      "queries/219510_noise.mp3 219510\n",
      "queries/136810_coding.mp3 136810\n",
      "queries/1400510_coding.mp3 1400510\n",
      "queries/887210_mobile.mp3 887210\n",
      "queries/1227910_noise.mp3 1227910\n",
      "queries/119410_mobile.mp3 119410\n",
      "queries/1269810_original.mp3 1269810\n",
      "queries/53310_noise.mp3 53310\n",
      "queries/136810_mobile.mp3 1065626\n",
      "queries/963810_mobile.mp3 963810\n",
      "queries/1390710_coding.mp3 1390710\n",
      "queries/1400510_mobile.mp3 1271442\n",
      "queries/119410_coding.mp3 119410\n",
      "queries/219510_original.mp3 219510\n",
      "queries/94710_noise.mp3 94710\n",
      "queries/963810_original.mp3 963810\n",
      "queries/1192210_original.mp3 1192210\n",
      "queries/152310_coding.mp3 152310\n",
      "queries/94710_original.mp3 94710\n",
      "queries/219510_mobile.mp3 1064503\n",
      "queries/810_coding.mp3 810\n",
      "queries/91810_mobile.mp3 91810\n",
      "queries/810_noise.mp3 810\n",
      "queries/219510_coding.mp3 219510\n",
      "queries/1400510_original.mp3 1400510\n",
      "queries/1227910_mobile.mp3 1227910\n",
      "queries/810_original.mp3 810\n",
      "queries/152310_noise.mp3 152310\n",
      "queries/1269810_noise.mp3 1269810\n",
      "queries/53310_coding.mp3 53310\n",
      "queries/1249910_mobile.mp3 1249910\n",
      "queries/94710_coding.mp3 94710\n",
      "queries/1390710_original.mp3 1390710\n",
      "queries/53310_mobile.mp3 53310\n",
      "queries/1249910_coding.mp3 1249910\n",
      "queries/152310_original.mp3 152310\n",
      "queries/242610_coding.mp3 242610\n",
      "queries/262010_noise.mp3 262010\n",
      "queries/1269810_coding.mp3 1269810\n",
      "queries/1147910_coding.mp3 1147910\n",
      "queries/119410_original.mp3 119410\n",
      "queries/810_mobile.mp3 1052255\n",
      "queries/119410_noise.mp3 119410\n",
      "queries/887210_original.mp3 887210\n",
      "queries/1084710_mobile.mp3 1356534\n",
      "queries/1147910_noise.mp3 1147910\n",
      "queries/136810_original.mp3 136810\n",
      "queries/887210_coding.mp3 887210\n",
      "queries/1249910_noise.mp3 1249910\n",
      "queries/94710_mobile.mp3 94710\n",
      "queries/262010_mobile.mp3 208032\n",
      "queries/242610_original.mp3 242610\n",
      "queries/136810_noise.mp3 136810\n",
      "queries/1227910_coding.mp3 1227910\n",
      "queries/963810_coding.mp3 963810\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "matches_3 = match_queries_with_chunks(\"queries\", \"partial_database2.pkl\", **config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9f29b-8265-4671-ae62-83e121856485",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_4 = match_queries(\"queries\", database_4, **config_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10d2c2-d4e2-41e4-9ac1-d97b53516996",
   "metadata": {},
   "source": [
    "# Task 3: Scale up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b337a1-8018-4260-b0c7-fd3da5d467f2",
   "metadata": {},
   "source": [
    "# Task 4: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
