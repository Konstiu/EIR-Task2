{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2ccad7-33bd-44b8-abb3-a0517c683e88",
   "metadata": {},
   "source": [
    "# MS 2.2- Group 10\n",
    "\n",
    "**Members**: Niklas Grüner (12217059), Konstantin Unterweger (12222169), Martin Harhammer (12221683)\n",
    "\n",
    "\n",
    "The following sections describe and implement an attempt to Audio Identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f54f9-5a1c-422d-a1b0-4a724d15679b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d41eb8e-5435-4235-be1c-2d82cf56a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import librosa\n",
    "#from scipy import signal\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "\n",
    "sys.path.append('..')\n",
    "import libfmp.b\n",
    "import libfmp.c2\n",
    "import libfmp.c6\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a404e0-ae94-4a93-90f6-1cb6f5d912ce",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Here we define all functions that, we later need to compute spectrograms, constellation maps, the hashes and perform the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37de749-9b67-444b-b8c0-51dd0c3b3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filenames(directory):\n",
    "    filenames = []\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Create the full path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if it is a file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Add to the dictionary, using the filename as the key\n",
    "            filenames.append(file_path)\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef06df91-9bc6-41cb-81ef-cd227c10a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrogram(fn_wav, Fs=22050, N=2048, H=1024, bin_max=128, frame_max=None, duration=None):\n",
    "    x, Fs = librosa.load(fn_wav, sr=Fs)\n",
    "    x_duration = len(x) / Fs\n",
    "    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n",
    "    if bin_max is None:\n",
    "        bin_max = X.shape[0]\n",
    "    if frame_max is None:\n",
    "        frame_max = X.shape[0]\n",
    "    Y = np.abs(X[:bin_max, :frame_max])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2700908-b75b-40c3-8248-156855ceebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_constellation_map(Y, dist_freq=7, dist_time=7, thresh=0.01):\n",
    "    \"\"\"Compute constellation map (implementation using image processing)\n",
    "\n",
    "    Notebook: C7/C7S1_AudioIdentification.ipynb\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): Spectrogram (magnitude)\n",
    "        dist_freq (int): Neighborhood parameter for frequency direction (kappa) (Default value = 7)\n",
    "        dist_time (int): Neighborhood parameter for time direction (tau) (Default value = 7)\n",
    "        thresh (float): Threshold parameter for minimal peak magnitude (Default value = 0.01)\n",
    "\n",
    "    Returns:\n",
    "        Cmap (np.ndarray): Boolean mask for peak structure (same size as Y)\n",
    "    \"\"\"\n",
    "    result = ndimage.maximum_filter(Y, size=[2*dist_freq+1, 2*dist_time+1], mode='constant')\n",
    "    Cmap = np.logical_and(Y == result, result > thresh)\n",
    "    return Cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "530cacda-d4b4-4302-aa6c-1dc2ea160662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def compute_constellation_map_single(args):\n",
    "    \"\"\"Compute the constellation map for a single file.\"\"\"\n",
    "    filename, dist_freq, dist_time = args\n",
    "    spectrogram = compute_spectrogram(filename)  # Perform I/O and computation\n",
    "    constellation_map = compute_constellation_map(spectrogram, dist_freq, dist_time)\n",
    "    return filename, constellation_map\n",
    "\n",
    "def compute_constellation_maps(filenames, dist_freq, dist_time):\n",
    "    \"\"\"Compute constellation maps using multithreading.\"\"\"\n",
    "    # Prepare arguments for each file\n",
    "    args = [(filename, dist_freq, dist_time) for filename in filenames]\n",
    "    \n",
    "    # Use ThreadPoolExecutor for multithreading\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(compute_constellation_map_single, args)\n",
    "    \n",
    "    # Convert results to a dictionary\n",
    "    Cmaps = dict(results)\n",
    "    return Cmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28bc56ad-2af7-49df-baa3-f4adfc094d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_database(\n",
    "    directory,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a fingerprint database from a dictionary of spectrograms.\n",
    "    \n",
    "    Args:\n",
    "        cmaps_D (dict): {track_name: 2D numpy array (boolean spectrogram)}\n",
    "        time_min_offset, time_max_offset, freq_min_offset, freq_max_offset (int): \n",
    "            Bounds for the \"target zone\" around an anchor point.\n",
    "    \n",
    "    Returns:\n",
    "        database (defaultdict): {hash: [(track_name, time_offset), ...]}\n",
    "    \"\"\"\n",
    "    tracks = load_filenames(directory) # load all track filenames\n",
    "    cmaps_D = compute_constellation_maps(tracks, 11, 3) # store the computed constellation maps for each configuration.\n",
    "    \n",
    "    database = defaultdict(list)\n",
    "    \n",
    "    for track_name, cmap in cmaps_D.items():\n",
    "        # Convert the boolean map to a list of points (freq, time), sorted by time\n",
    "        point_list = sorted(np.argwhere(cmap).tolist(), key=lambda x: x[1])\n",
    "\n",
    "        for anchor in point_list:\n",
    "            # Find all points in the anchor's target zone\n",
    "            target_points = get_target_zone_points(\n",
    "                anchor, \n",
    "                point_list,\n",
    "                time_min_offset=time_min_offset,\n",
    "                time_max_offset=time_max_offset,\n",
    "                freq_min_offset=freq_min_offset,\n",
    "                freq_max_offset=freq_max_offset\n",
    "            )\n",
    "\n",
    "            # Compute hash for each (anchor, target) pair\n",
    "            for target_point in target_points:\n",
    "                h = compute_hash(anchor, target_point)\n",
    "                database[h].append((track_name, anchor[1]))\n",
    "    \n",
    "    return database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4636f6d5-0fbe-46b6-9222-17b58ab9f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(anchor, target):\n",
    "    \"\"\"Generate a 32-bit hash.\"\"\"\n",
    "    f1, t1 = anchor\n",
    "    f2, t2 = target\n",
    "    dt = t2 - t1\n",
    "    return (f1 & 0x3FF) | ((f2 & 0x3FF) << 10) | ((dt & 0xFFF) << 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c2067f6-84be-431a-913b-0cec6237fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_zone_points(anchor, point_list,\n",
    "                           time_min_offset, time_max_offset,\n",
    "                           freq_min_offset, freq_max_offset):\n",
    "    \"\"\"\n",
    "    Finds points in 'point_list' that lie within time [t1 + time_min_offset, t1 + time_max_offset]\n",
    "    and frequency [f1 + freq_min_offset, f1 + freq_max_offset].\n",
    "    \n",
    "    anchor: (f1, t1) or (t1, f1) – whichever convention you are using.\n",
    "    point_list: list of (f, t) or (t, f) – must be sorted by the time coordinate if we want to break early.\n",
    "    time_min_offset, time_max_offset: how far in time we look relative to anchor's time t1.\n",
    "    freq_min_offset, freq_max_offset: how far in frequency we look relative to anchor's freq f1.\n",
    "    \"\"\"\n",
    "    f1, t1 = anchor\n",
    "    \n",
    "    # Time bounds\n",
    "    t_min = t1 + time_min_offset\n",
    "    t_max = t1 + time_max_offset\n",
    "    \n",
    "    # Frequency bounds\n",
    "    f_min = f1 + freq_min_offset\n",
    "    f_max = f1 + freq_max_offset\n",
    "    \n",
    "    target_zone_points = []\n",
    "    for (f2, t2) in point_list:\n",
    "        # If the list is sorted by time and t2 > t_max, we can break early.\n",
    "        if t2 > t_max:\n",
    "            break\n",
    "        \n",
    "        if t_min < t2 <= t_max and f_min <= f2 <= f_max:\n",
    "            target_zone_points.append((f2, t2))\n",
    "    \n",
    "    return target_zone_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3fd158-c0ba-461c-95f1-ecec90d31fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def extract_numeric_id(filename):\n",
    "    \"\"\"\n",
    "    Example: \n",
    "      - 'queries/1269810_original.mp3' => '1269810'\n",
    "      - 'tracks/1269810.mp3'           => '1269810'\n",
    "    Adjust to your naming conventions as needed.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e7874a6-1d57-42a5-b18e-eef2fd2a243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_for_query(\n",
    "    query_name,\n",
    "    cmap,\n",
    "    database,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a query's spectrogram cmap (2D Boolean array),\n",
    "    find the best matching track in the database of fingerprints.\n",
    "\n",
    "    Returns:\n",
    "        best_track: str or None\n",
    "        best_delta: int or None\n",
    "        best_count: int\n",
    "        point_list: list of non-zero (freq,time) points in the query\n",
    "    \"\"\"\n",
    "    # Convert the boolean array to a list of points (freq, time)\n",
    "    point_list = sorted(np.argwhere(cmap).tolist(), key=lambda x: x[1])\n",
    "\n",
    "    # Dictionary of track_name -> (offset -> count)\n",
    "    matches = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # For each point in the query, find matching points in the database\n",
    "    for anchor in point_list:\n",
    "        target_points = get_target_zone_points(\n",
    "            anchor,\n",
    "            point_list,\n",
    "            time_min_offset=time_min_offset,\n",
    "            time_max_offset=time_max_offset,\n",
    "            freq_min_offset=freq_min_offset,\n",
    "            freq_max_offset=freq_max_offset\n",
    "        )\n",
    "\n",
    "        for target_point in target_points:\n",
    "            h = compute_hash(anchor, target_point)\n",
    "\n",
    "            # If our hash is found in the database\n",
    "            if h in database:\n",
    "                # database[h] = list of (track_name, track_offset)\n",
    "                for track_name, track_offset in database[h]:\n",
    "                    delta_offset = track_offset - anchor[1]\n",
    "                    matches[track_name][delta_offset] += 1\n",
    "    \n",
    "    # Find the best match with the highest count\n",
    "    best_track = None\n",
    "    best_delta = None\n",
    "    best_count = 0\n",
    "    \n",
    "    for track_name, offset_counts in matches.items():\n",
    "        for delta_offset, count in offset_counts.items():\n",
    "            if count > best_count:\n",
    "                best_count = count\n",
    "                best_track = track_name\n",
    "                best_delta = delta_offset\n",
    "\n",
    "    return best_track, best_delta, best_count, point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b8fa4b0-274a-4f67-b712-017a5dee45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_queries(\n",
    "    directory,\n",
    "    database,\n",
    "    time_min_offset=5,\n",
    "    time_max_offset=30,\n",
    "    freq_min_offset=-15,\n",
    "    freq_max_offset=15\n",
    "):\n",
    "    \"\"\"\n",
    "    - Matches each query in cmaps_Q to the best track in the database.\n",
    "    :param cmaps_Q: dict {query_name: 2D numpy array (spectrogram boolean map)}\n",
    "    :param database: dict {hash: [(track_name, track_offset), ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    queries = load_filenames(directory) # load all track filenames\n",
    "        \n",
    "    results = []\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Collect queries in a list to iterate consistently\n",
    "    total_queries = len(queries)\n",
    "\n",
    "    \n",
    "\n",
    "    for query_name in queries:\n",
    "        starttime = time.time()\n",
    "        \n",
    "        _, cmap = compute_constellation_map_single((query_name, 11, 3))       \n",
    "\n",
    "        \n",
    "        best_track, best_delta, best_count, point_list = find_best_match_for_query(\n",
    "            query_name,\n",
    "            cmap,\n",
    "            database,\n",
    "            time_min_offset=time_min_offset,\n",
    "            time_max_offset=time_max_offset,\n",
    "            freq_min_offset=freq_min_offset,\n",
    "            freq_max_offset=freq_max_offset\n",
    "        )\n",
    "\n",
    "        endtime = time.time()\n",
    "        \n",
    "\n",
    "        query_id = extract_numeric_id(query_name)\n",
    "        track_id = extract_numeric_id(best_track) if best_track else None\n",
    "        correct = (query_id == track_id)\n",
    "        if correct:\n",
    "            num_correct += 1\n",
    "\n",
    "        # Store the result\n",
    "        results.append((query_name, best_track, best_delta, best_count, correct, (endtime-starttime)*1000))\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Print final results\n",
    "    # ----------------------------------------------------------------------\n",
    "    accuracy = num_correct / total_queries if total_queries else 0\n",
    "    print(\"\\nMATCHING RESULTS:\")\n",
    "    sum_duration = 0\n",
    "    for qname, tname, offset, count, is_correct, duration in results:\n",
    "        print(f\"Query: {qname} => Best match: {tname}, Time: {duration} ms, Offset: {offset}, Count: {count}, Correct: {is_correct}\")\n",
    "        sum_duration += duration\n",
    "\n",
    "    print(f\"\\nCorrect matches: {num_correct}/{total_queries}\")\n",
    "    print(f\"Average query time: {sum_duration/len(results)} ms\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9cf99-83c1-4495-aa81-895789d3c546",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6e30e-bdb3-4691-9050-a712fcf7b579",
   "metadata": {},
   "source": [
    "## Define configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cecf6808-828d-4ea2-9133-8fa244391291",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    \"time_min_offset\": 5,\n",
    "    \"time_max_offset\": 30,\n",
    "    \"freq_min_offset\": -15,\n",
    "    \"freq_max_offset\": 15\n",
    "}\n",
    "\n",
    "config_2 = {\n",
    "    \"time_min_offset\": 0,\n",
    "    \"time_max_offset\": 30,\n",
    "    \"freq_min_offset\": 0,\n",
    "    \"freq_max_offset\": 25\n",
    "}\n",
    "\n",
    "config_3 = {\n",
    "    \"time_min_offset\": 10,\n",
    "    \"time_max_offset\": 30,\n",
    "    \"freq_min_offset\": 0,\n",
    "    \"freq_max_offset\": 30\n",
    "}\n",
    "\n",
    "config_4 = {\n",
    "    \"time_min_offset\": 5,\n",
    "    \"time_max_offset\": 10,\n",
    "    \"freq_min_offset\": -5,\n",
    "    \"freq_max_offset\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554b41e-ef1f-446d-aa7e-19af3ccba652",
   "metadata": {},
   "source": [
    "## Build the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b7e400-1f3b-4327-89ab-6d64f32c6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.94036388397217\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "database_1 = build_database(\"tracks2\", **config_1)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147d027-522b-48dd-8507-797722ff18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_2 = build_database(\"tracks\", **config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946fddc-2a3b-4edd-9166-fe1337b3a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_3 = build_database(\"tracks\", **config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc4578-d745-4daf-90ea-07a53c3dcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_4 = build_database(\"tracks\", **config_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57670e89-9d39-4e92-a9a4-cf674a13f30a",
   "metadata": {},
   "source": [
    "### Store the databases on the harddrive (skip if not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0820b0f6-341a-4779-b941-1c2007cfc143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database_2 doesn't exist\n",
      "database_3 doesn't exist\n",
      "database_4 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "try:\n",
    "    database_1\n",
    "    with open(\"database_1.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_1, file)\n",
    "except NameError:\n",
    "    print(\"database_1 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_2\n",
    "    with open(\"database_2.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_2, file)\n",
    "except NameError:\n",
    "    print(\"database_2 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_2\n",
    "    with open(\"database_3.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_2, file)\n",
    "except NameError:\n",
    "    print(\"database_3 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    database_4\n",
    "    with open(\"database_4.pkl\", \"wb\") as file:\n",
    "        pickle.dump(database_4, file)\n",
    "except NameError:\n",
    "    print(\"database_4 doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01437-7b86-49f8-94df-b8bcc61367a2",
   "metadata": {},
   "source": [
    "# Task 2: Audio Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564c111-4dc1-44df-8fea-6f73b4a3e2a4",
   "metadata": {},
   "source": [
    "### Load the Databases into memory (skip if not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03698546-ce85-434d-bb94-4fb192a0344e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'database_2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1617/1866290084.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"database_2.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdatabase_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'database_2.pkl'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"database_1.pkl\", \"rb\") as file:\n",
    "        database_1 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_1 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_2.pkl\", \"rb\") as file:\n",
    "        database_2 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_2 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_3.pkl\", \"rb\") as file:\n",
    "        database_3 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_3 doesn't exist\")\n",
    "\n",
    "try:\n",
    "    with open(\"database_4.pkl\", \"rb\") as file:\n",
    "        database_4 = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"database_4 doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2a224-01d3-4882-940d-1dc0ad204d07",
   "metadata": {},
   "source": [
    "### Match all test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554723cb-6157-45c4-a28b-20f1e2da7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "matches_1 = match_queries(\"queries\", database_1, **config_1)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8dd84-c409-4f64-a133-d9f7309884b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = match_queries(\"queries\", database_2, **config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26c43c-047d-4d50-8286-efb526123346",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_3 = match_queries(\"queries\", database_3, **config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9f29b-8265-4671-ae62-83e121856485",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_4 = match_queries(\"queries\", database_4, **config_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10d2c2-d4e2-41e4-9ac1-d97b53516996",
   "metadata": {},
   "source": [
    "# Task 3: Scale up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b337a1-8018-4260-b0c7-fd3da5d467f2",
   "metadata": {},
   "source": [
    "# Task 4: Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
